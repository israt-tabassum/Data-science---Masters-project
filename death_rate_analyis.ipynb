{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c202e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----------+-----------+-----------+\n",
      "|        Country Name|       2017|       2018|       2019|       2020|\n",
      "+--------------------+-----------+-----------+-----------+-----------+\n",
      "|               Aruba|      8.907|      9.053|      9.205|      9.362|\n",
      "|Africa Eastern an...|7.691457313|7.521437993| 7.37664367|7.250955605|\n",
      "|         Afghanistan|      6.575|      6.423|      6.285|      6.157|\n",
      "|Africa Western an...|10.39363761|10.16649162| 9.96084289|9.774667737|\n",
      "|              Angola|      8.432|       8.19|      7.981|      7.798|\n",
      "|             Albania|      7.714|      7.898|      8.082|      8.263|\n",
      "|             Andorra|       null|        4.4|        3.9|       null|\n",
      "|          Arab World|5.413429222|5.380288079|5.352157196|5.329767688|\n",
      "+--------------------+-----------+-----------+-----------+-----------+\n",
      "only showing top 8 rows\n",
      "\n",
      "root\n",
      " |-- Country Name: string (nullable = true)\n",
      " |-- 2017: double (nullable = true)\n",
      " |-- 2018: double (nullable = true)\n",
      " |-- 2019: double (nullable = true)\n",
      " |-- 2020: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#Configuration\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local[2]')\\\n",
    "    .appName('quake_etl')\\\n",
    "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:2.4.1')\\\n",
    "    .getOrCreate() \n",
    "#load data\n",
    "death_2 = spark.read.csv(r\"C:\\Users\\israt\\OneDrive\\Desktop\\DM_Project\\project\\death_2.csv\", header=True)\n",
    "\n",
    "\n",
    "#Delete column\n",
    "delete_columns_2 = ['1960', '1961','1962','1963','1964','1965','1966','1967','1968','1969','1970', '1971','1972','1973','1974','1975','1976','1977','1978','1979','1980','1981','1982','1983','1984','1985','1986','1987','1988','1989','1990','1991','1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2021','Country Code','Indicator Name','Indicator Code']\n",
    "death_2 = death_2.drop(*delete_columns_2)\n",
    "death_2.show(8)\n",
    "\n",
    "#convert data type string to double\n",
    "death_2 = death_2.withColumn('2017',death_2['2017'].cast(DoubleType())).withColumn('2018',death_2['2018'].cast(DoubleType())).withColumn('2019',death_2['2019'].cast(DoubleType())).withColumn('2020',death_2['2020'].cast(DoubleType()))\n",
    "death_2.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "# connect with mongodb by creating database\n",
    "death_2.write.format('mongo')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('spark.mongodb.output.uri','mongodb://127.0.0.1/death_2.death_2').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28dc013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be521a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
